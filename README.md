# **TIKLocal Copilot - Extension VSCode**  

üöÄ **G√©n√©rez et modifiez du code avec des mod√®les IA locaux (Ollama)** directement dans VSCode, sans d√©pendre du cloud !  

## **Fonctionnalit√©s** ‚ú®  
‚úî **G√©n√©ration de code contextuelle** (selon la s√©lection ou le fichier entier)  
‚úî **Modification intelligente** du code existant  
‚úî **Chat interactif** avec historique des conversations  
‚úî **Support multi-langage** (Python, JavaScript, HTML, etc.)  
‚úî **100% local** (utilise Ollama pour ex√©cuter des mod√®les comme StarCoder, CodeLlama, etc.)  
‚úî **Bouton "Appliquer"** pour ins√©rer le code g√©n√©r√© en un clic  

---

## **Pr√©requis** ‚öôÔ∏è  
1. **Ollama** install√© ([T√©l√©charger ici](https://ollama.ai/))  
2. **Mod√®les IA** t√©l√©charg√©s (ex: `starcoder`, `deepseek-coder`, `codellama`)  

### **Installation d'Ollama**  
```bash
curl -fsSL https://ollama.com/install.sh | sh  # Linux/Mac
winget install ollama                          # Windows (via Winget)
```

### **T√©l√©charger un mod√®le**  
```bash
ollama pull starcoder
```

---

## **Installation dans VSCode** üì•  

### **Option 1 : Depuis le Marketplace (recommand√©)**  
1. Ouvrez VSCode  
2. Allez dans **Extensions** (Ctrl+Shift+X)  
3. Cherchez **"Local Copilot"**  
4. Cliquez sur **Installer**  

### **Option 2 : Manuellement (fichier .vsix)**  
1. T√©l√©chargez la derni√®re version `.vsix` depuis [Releases](https://github.com/votre-repo/releases)  
2. Dans VSCode :  
   ```bash
   code --install-extension chemin/vers/local-copilot-X.X.X.vsix
   ```
3. **Red√©marrez VSCode**  

---

## **Utilisation** üõ†Ô∏è  

1. **Ouvrez un fichier de code**  
2. **S√©lectionnez du code** (ou laissez vide pour tout le fichier)  
3. **Lancez le chat** :  
   - **Commande palette** (Ctrl+Shift+P) ‚Üí `Local Copilot: Ouvrir le chat`  
   - **Raccourci clavier** : `Ctrl+Alt+C`  
4. **Posez votre question** (ex: *"Corrige ce code"*, *"Optimise cette fonction"*)  
5. **Cliquez sur "Appliquer"** sous la r√©ponse pour ins√©rer le code g√©n√©r√©  

![Demo](images/demo.gif)  

---

## **Configuration** ‚öôÔ∏è  

### **Changer de mod√®le IA**  
1. Ouvrez le chat (`Ctrl+Alt+C`)  
2. Cliquez sur **"Mod√®le: [nom-du-mod√®le]"** en haut √† droite  
3. S√©lectionnez un mod√®le install√© (ex: `starcoder`, `phi3`)  

### **Param√®tres recommand√©s** (dans `settings.json`)  
```json
{
  "localCopilot.defaultModel": "starcoder",
  "localCopilot.temperature": 0.3  // Contr√¥le la cr√©ativit√© (0=pr√©cis, 1=cr√©atif)
}
```

---

## **D√©pannage** üêõ  

### **Probl√®me : L'extension ne r√©pond pas**  
‚úÖ V√©rifiez qu'Ollama tourne :  
```bash
ollama serve  # Doit √™tre lanc√© en arri√®re-plan
```  

### **Probl√®me : Pas de r√©ponse de l'IA**  
‚úÖ V√©rifiez les logs :  
1. Ouvrez la palette (Ctrl+Shift+P) ‚Üí `Local Copilot: Afficher les logs`  
2. V√©rifiez le fichier `.vscode/local-copilot.log` dans votre projet  

### **Probl√®me : Mod√®le introuvable**  
‚úÖ T√©l√©chargez-le via Ollama :  
```bash
ollama pull nom_du_mod√®le  # Ex: ollama pull codellama
```

---

## **D√©veloppement** üë®‚Äçüíª  
Pour contribuer ou compiler depuis les sources :  
```bash
git clone https://github.com/votre-repo/local-copilot.git
cd local-copilot
npm install
npm run compile
code .
```

---

## **Licence** üìú  
MIT License - [Lire ici](LICENSE)  
